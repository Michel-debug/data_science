{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening the box of gradient boosting decision trees (GBDT)\n",
    "\n",
    "```\n",
    "Authors: Alexandre Gramfort\n",
    "         Thomas Moreau\n",
    "```\n",
    "\n",
    "The aim of this notebook is 2-folds:\n",
    "\n",
    "- dissecting a pure Python (unoptimized) implementation of GBRT\n",
    "- use a profiler (here snakeviz) to identify the computational bottleneck.\n",
    "\n",
    "We will explore the code in the file `tinygbt.py`.\n",
    "\n",
    "TinyGBT (Tiny Gradient Boosted Trees) is a 200 line gradient boosted trees implementation written in pure Python.\n",
    "\n",
    "First let's load some regression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv('datasets/regression.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('datasets/regression.test', header=None, sep='\\t')\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "y = df[0].values\n",
    "X = df.drop(0, axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data in train and test. Here the test data will be\n",
    "used as **validation set** to do **\"early stopping\"** i.e. to stop adding trees\n",
    "to the model if the validation loss increases for multiple successive boosting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's run our TinyGBT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "Iter   0, Train's L2: 0.3326061855, Valid's L2: 0.3329401346, Elapsed: 1.78 secs\n",
      "Iter   1, Train's L2: 0.1925295668, Valid's L2: 0.2339036288, Elapsed: 1.77 secs\n",
      "Iter   2, Train's L2: 0.1837464009, Valid's L2: 0.2295993444, Elapsed: 1.78 secs\n",
      "Iter   3, Train's L2: 0.1815630268, Valid's L2: 0.2281974171, Elapsed: 1.78 secs\n",
      "Iter   4, Train's L2: 0.1809400833, Valid's L2: 0.2278910397, Elapsed: 1.80 secs\n",
      "Iter   5, Train's L2: 0.1807753271, Valid's L2: 0.2278302139, Elapsed: 1.81 secs\n",
      "Iter   6, Train's L2: 0.1807266076, Valid's L2: 0.2278130572, Elapsed: 1.80 secs\n",
      "Iter   7, Train's L2: 0.1807121121, Valid's L2: 0.2278076097, Elapsed: 1.83 secs\n",
      "Iter   8, Train's L2: 0.1807077695, Valid's L2: 0.2278060130, Elapsed: 1.85 secs\n",
      "Iter   9, Train's L2: 0.1807064624, Valid's L2: 0.2278055556, Elapsed: 1.84 secs\n",
      "Training finished. Elapsed: 18.05 secs\n",
      "Start predicting...\n",
      "The MSE of prediction is: 0.2278055555875028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from tinygbt import GBT\n",
    "\n",
    "print('Start training...')\n",
    "gbt = GBT(n_estimators=10, gamma=0., lambd=1,\n",
    "          min_split_gain=0.1, max_depth=5,\n",
    "          learning_rate=0.3)\n",
    "gbt.fit(X_train, y_train, valid_set=(X_test, y_test), early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "y_pred = gbt.predict(X_test)\n",
    "print('The MSE of prediction is:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make sure this code gives comparable results with [lightgbm](https://lightgbm.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb Cellule 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightgbm\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlgb\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# create dataset for lightgbm\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lgb_train \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(X_train, y_train)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 1.,\n",
    "    'bagging_fraction': 1.,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "rmse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'The MSE of prediction is: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also with scikit-learn [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb Cellule 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# from sklearn.experimental import enable_hist_gradient_boosting  # uncomment this for sklearn < 1.0\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m HistGradientBoostingRegressor\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m est \u001b[39m=\u001b[39m HistGradientBoostingRegressor(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     max_iter\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     validation_fraction\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     l2_regularization\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     min_samples_leaf\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     n_iter_no_change\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y_pred \u001b[39m=\u001b[39m est\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m rmse_test \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# from sklearn.experimental import enable_hist_gradient_boosting  # uncomment this for sklearn < 1.0\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "est = HistGradientBoostingRegressor(\n",
    "    max_iter=10,\n",
    "    validation_fraction=0.2,\n",
    "    random_state=42,\n",
    "    l2_regularization=0.,\n",
    "    min_samples_leaf=20,\n",
    "    learning_rate=0.3,\n",
    "    n_iter_no_change=5).fit(X_train, y_train)\n",
    "y_pred = est.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'The MSE of prediction is: {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a round of profiling using snakeviz\n",
    "\n",
    "While lightgbm is implemented in C++ and HistGradientBoostingRegressor with Cython, our TinyGBT is implemented in pure Python. The consequence is that our code is much slower. To identify the computational bottleneck let's profile our code. For this we will use snakeviz.\n",
    "\n",
    "There are many other profilers such as:\n",
    "- line_profiler: https://github.com/pyutils/line_profiler (easy line by line profiling for Python code)\n",
    "- viztracer: https://viztracer.readthedocs.io (allows to profile low level code including compiled Cython code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snakeviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb Cellule 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mload_ext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msnakeviz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m gbt \u001b[39m=\u001b[39m GBT(n_estimators\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,  \u001b[39m# do only two rounds of boosting\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m           gamma\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m, lambd\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m           min_split_gain\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m           learning_rate\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenchenjunjie/M1_saclay/datacamp/datacamp-master/09_trees_gradient_boosting/02-gradient_boosting_pure_python.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39msnakeviz\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgbt.fit(X_train, y_train, valid_set=(X_test, y_test), early_stopping_rounds=5)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2414\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2412\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2413\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2414\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2416\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2417\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2418\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[39mraise\u001b[39;00m UsageError(\u001b[39m'\u001b[39m\u001b[39mMissing module name.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshell\u001b[39m.\u001b[39mextension_manager\u001b[39m.\u001b[39mload_extension(module_str)\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39malready loaded\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m extension is already loaded. To reload it, use:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/extensions.py:76\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_extension(module_str)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m module_str \u001b[39min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/extensions.py:91\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshell\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m module_str \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[0;32m---> 91\u001b[0m         mod \u001b[39m=\u001b[39m import_module(module_str)\n\u001b[1;32m     92\u001b[0m     mod \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[module_str]\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snakeviz'"
     ]
    }
   ],
   "source": [
    "%load_ext snakeviz\n",
    "\n",
    "gbt = GBT(n_estimators=2,  # do only two rounds of boosting\n",
    "          gamma=0., lambd=1,\n",
    "          min_split_gain=0.1, max_depth=5,\n",
    "          learning_rate=0.3)\n",
    "\n",
    "%snakeviz gbt.fit(X_train, y_train, valid_set=(X_test, y_test), early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "         <li>Where is most of the time spent?</li>\n",
    "         <li>What is the complexity of the <code>TreeNode.build</code> method as a function of the number of samples and the number of features?</li>\n",
    "         <li>What could we do about this?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
